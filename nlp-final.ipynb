{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 12467968,
     "sourceType": "datasetVersion",
     "datasetId": 7865712
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\nimport json\nimport random\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:36.961069Z",
     "start_time": "2025-07-23T11:28:32.924606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\code\\attention_tracker_clone\\Attention-Tracker\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:38.551484Z",
     "start_time": "2025-07-23T11:28:38.548447Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "class Model:\n    def __init__(self, config):\n        self.provider = config[\"model_info\"][\"provider\"]\n        self.name = config[\"model_info\"][\"name\"]\n        self.temperature = float(config[\"params\"][\"temperature\"])\n\n    def print_model_info(self):\n        print(f\"{'-'*len(f'| Model name: {self.name}')}\\n| Provider: {self.provider}\\n| Model name: {self.name}\\n{'-'*len(f'| Model name: {self.name}')}\")\n\n    def set_API_key(self):\n        raise NotImplementedError(\"ERROR: Interface doesn't have the implementation for set_API_key\")\n    \n    def query(self):\n        raise NotImplementedError(\"ERROR: Interface doesn't have the implementation for query\")",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:40.267568Z",
     "start_time": "2025-07-23T11:28:40.263303Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "class AttentionModel(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.name = config[\"model_info\"][\"name\"]\n",
    "        self.max_output_tokens = int(config[\"params\"][\"max_output_tokens\"])\n",
    "        model_id = config[\"model_info\"][\"model_id\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map = device,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"eager\",\n",
    "        ).eval()\n",
    "\n",
    "        self.top_k = 50\n",
    "        self.top_p = None\n",
    "\n",
    "        if config[\"params\"][\"important_heads\"] == \"all\":\n",
    "            attn_size = self.get_map_dim()\n",
    "            self.important_heads = [[i, j] for i in range(\n",
    "                attn_size[0]) for j in range(attn_size[1])]\n",
    "        else:\n",
    "            self.important_heads = config[\"params\"][\"important_heads\"]\n",
    "\n",
    "\n",
    "    def get_map_dim(self):\n",
    "        _, _, attention_maps, _, _, _ = self.inference(\"print hi\", \"\")\n",
    "        attention_map = attention_maps[0]\n",
    "        return len(attention_map), attention_map[0].shape[1]\n",
    "\n",
    "    def inference(self, instruction, data, max_output_tokens=None):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"Data: \" + data}\n",
    "        ]\n",
    "\n",
    "        # Use tokenization with minimal overhead\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        instruction_len = len(self.tokenizer.encode(instruction))\n",
    "        data_len = len(self.tokenizer.encode(data))\n",
    "\n",
    "        model_inputs = self.tokenizer(\n",
    "            [text], return_tensors=\"pt\").to(self.model.device)\n",
    "        input_tokens = self.tokenizer.convert_ids_to_tokens(\n",
    "            model_inputs['input_ids'][0])\n",
    "\n",
    "        # find the data token positions\n",
    "        if \"qwen\" in self.name:\n",
    "            data_range = ((3, 3+instruction_len), (-5-data_len, -5))\n",
    "        elif \"phi3\" in self.name:\n",
    "            data_range = ((1, 1+instruction_len), (-2-data_len, -2))\n",
    "        elif \"granite3-8b\" in self.name:\n",
    "            data_range = ((3, 3+instruction_len), (-5-data_len, -5))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        generated_tokens = []\n",
    "        generated_probs = []\n",
    "        input_ids = model_inputs.input_ids\n",
    "        attention_mask = model_inputs.attention_mask\n",
    "\n",
    "        attention_maps = []\n",
    "\n",
    "        if max_output_tokens != None:\n",
    "            n_tokens = max_output_tokens\n",
    "        else:\n",
    "            n_tokens = self.max_output_tokens\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_tokens):\n",
    "                output = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_attentions=True\n",
    "                )\n",
    "\n",
    "                logits = output.logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # next_token_id = logits.argmax(dim=-1).squeeze()\n",
    "                next_token_id = sample_token(\n",
    "                    logits[0], top_k=self.top_k, top_p=self.top_p, temperature=1.0)[0]\n",
    "\n",
    "                generated_probs.append(probs[0, next_token_id.item()].item())\n",
    "                generated_tokens.append(next_token_id.item())\n",
    "\n",
    "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, next_token_id.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
    "                attention_mask = torch.cat(\n",
    "                    (attention_mask, torch.tensor([[1]], device=input_ids.device)), dim=-1)\n",
    "\n",
    "                attention_map = [attention.detach().cpu().half()\n",
    "                                 for attention in output['attentions']]\n",
    "                attention_map = [torch.nan_to_num(\n",
    "                    attention, nan=0.0) for attention in attention_map]\n",
    "                attention_map = get_last_attn(attention_map)\n",
    "                attention_maps.append(attention_map)\n",
    "\n",
    "        output_tokens = [self.tokenizer.decode(\n",
    "            token, skip_special_tokens=True) for token in generated_tokens]\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return generated_text, output_tokens, attention_maps, input_tokens, data_range, generated_probs\n"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:44.412588Z",
     "start_time": "2025-07-23T11:28:41.654946Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "def open_config(config_path):\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    return config\n\ndef create_model(config):\n    provider = config[\"model_info\"][\"provider\"].lower()\n    if provider == 'attn-hf':\n        model = AttentionModel(config)\n    else:\n        raise ValueError(f\"ERROR: Unknown provider {provider}\")\n    return model",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:47.836483Z",
     "start_time": "2025-07-23T11:28:47.830963Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "class AttentionDetector():\n    def __init__(self, model, pos_examples=None, neg_examples=None, use_token=\"first\", instruction=\"Say xxxxxx\", threshold=0.5):\n        self.name = \"attention\"\n        self.attn_func = \"normalize_sum\"\n        self.model = model\n        self.important_heads = model.important_heads\n        self.instruction = instruction\n        self.use_token = use_token\n        self.threshold = threshold\n        if pos_examples and neg_examples:\n            pos_scores, neg_scores = [], []\n            for prompt in pos_examples:\n                _, _, attention_maps, _, input_range, generated_probs = self.model.query(\n                    prompt, return_type=\"attention\")\n                pos_scores.append(self.attn2score(attention_maps, input_range))\n\n            for prompt in neg_examples:\n                _, _, attention_maps, _, input_range, generated_probs = self.model.query(\n                    prompt, return_type=\"attention\")\n                neg_scores.append(self.attn2score(attention_maps, input_range))\n\n            self.threshold = np.mean(neg_scores)\n\n        if pos_examples and not neg_examples:\n            pos_scores = []\n            for prompt in pos_examples:\n                _, _, attention_maps, _, input_range, generated_probs = self.model.query(\n                    prompt, return_type=\"attention\")\n                pos_scores.append(self.attn2score(attention_maps, input_range))\n\n            self.threshold = np.mean(pos_scores) - 4 * np.std(pos_scores)\n\n    def attn2score(self, attention_maps, input_range):\n        if self.use_token == \"first\":\n            attention_maps = [attention_maps[0]]\n\n        scores = []\n        for attention_map in attention_maps:\n            heatmap = process_attn(\n                attention_map, input_range, self.attn_func)\n            score = calc_attn_score(heatmap, self.important_heads)\n            scores.append(score)\n\n        return sum(scores) if len(scores) > 0 else 0\n\n    def detect(self, data_prompt):\n        _, _, attention_maps, _, input_range, _ = self.model.inference(\n            self.instruction, data_prompt, max_output_tokens=1)\n\n        focus_score = self.attn2score(attention_maps, input_range)\n        return bool(focus_score <= self.threshold), {\"focus_score\": focus_score}\n",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:49.113216Z",
     "start_time": "2025-07-23T11:28:49.106272Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "def get_last_attn(attn_map):\n    for i, layer in enumerate(attn_map):\n        attn_map[i] = layer[:, :, -1, :].unsqueeze(2)\n\n    return attn_map\n\ndef sample_token(logits, top_k=None, top_p=None, temperature=1.0):\n    # Optionally apply temperature\n    logits = logits / temperature\n\n    # Apply top-k sampling\n    if top_k is not None:\n        top_k = min(top_k, logits.size(-1))  # Ensure top_k <= vocab size\n        values, indices = torch.topk(logits, top_k)\n        probs = F.softmax(values, dim=-1)\n        next_token_id = indices[torch.multinomial(probs, 1)]\n\n        return next_token_id\n\n    return logits.argmax(dim=-1).squeeze()",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:52.949919Z",
     "start_time": "2025-07-23T11:28:52.945449Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "def process_attn(attention, rng, attn_func):\n    heatmap = np.zeros((len(attention), attention[0].shape[1]))\n    for i, attn_layer in enumerate(attention):\n        attn_layer = attn_layer.to(torch.float32).numpy()\n\n        if \"sum\" in attn_func:\n            last_token_attn_to_inst = np.sum(attn_layer[0, :, -1, rng[0][0]:rng[0][1]], axis=1)\n            attn = last_token_attn_to_inst\n        \n        elif \"max\" in attn_func:\n            last_token_attn_to_inst = np.max(attn_layer[0, :, -1, rng[0][0]:rng[0][1]], axis=1)\n            attn = last_token_attn_to_inst\n\n        else: raise NotImplementedError\n            \n        last_token_attn_to_inst_sum = np.sum(attn_layer[0, :, -1, rng[0][0]:rng[0][1]], axis=1)\n        last_token_attn_to_data_sum = np.sum(attn_layer[0, :, -1, rng[1][0]:rng[1][1]], axis=1)\n\n        if \"normalize\" in attn_func:\n            epsilon = 1e-8\n            heatmap[i, :] = attn / (last_token_attn_to_inst_sum + last_token_attn_to_data_sum + epsilon)\n        else:\n            heatmap[i, :] = attn\n\n    heatmap = np.nan_to_num(heatmap, nan=0.0)\n\n    return heatmap\n\n\ndef calc_attn_score(heatmap, heads):\n    score = np.mean([heatmap[l, h] for l, h in heads], axis=0)\n    return score\n\n",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:28:54.295714Z",
     "start_time": "2025-07-23T11:28:54.286435Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    output_logs = f\"./result/{args.dataset_name}/{args.model_name}-{args.seed}.json\"\n",
    "    output_result = f\"./result/{args.dataset_name}/result.jsonl\"\n",
    "    \n",
    "    model_config_path = f\"configs/model_configs/qwen2-attn_config.json\"\n",
    "    model_config = open_config(config_path=model_config_path)\n",
    "\n",
    "    model = create_model(config=model_config)\n",
    "    model.print_model_info()\n",
    "\n",
    "    dataset = load_dataset(\"deepset/prompt-injections\")\n",
    "    test_data = dataset['test']\n",
    "    \n",
    "    detector = AttentionDetector(model)\n",
    "    print(\"===================\")\n",
    "    print(f\"Using detector: {detector.name}\")\n",
    "\n",
    "    labels, predictions, scores = [], [], []\n",
    "    logs = []\n",
    "\n",
    "    for data in tqdm(test_data):\n",
    "        result = detector.detect(data['text'])\n",
    "        detect = result[0]\n",
    "        score = result[1]['focus_score']\n",
    "\n",
    "        labels.append(data['label'])\n",
    "        predictions.append(detect)\n",
    "        scores.append(1-score)\n",
    "\n",
    "        result_data = {\n",
    "            \"text\": data['text'],\n",
    "            \"label\": data['label'],\n",
    "            \"result\": result\n",
    "        }\n",
    "\n",
    "        logs.append(result_data)\n",
    "\n",
    "    auc_score = roc_auc_score(labels, scores)\n",
    "    auprc_score = average_precision_score(labels, scores)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    auc_score = round(auc_score, 3)\n",
    "    auprc_score = round(auprc_score, 3)\n",
    "    fnr = round(fnr, 3)\n",
    "    fpr = round(fpr, 3)\n",
    "\n",
    "    print(f\"AUC Score: {auc_score}; AUPRC Score: {auprc_score}; FNR: {fnr}; FPR: {fpr}\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_logs), exist_ok=True)\n",
    "    with open(output_logs, \"w\") as f_out:\n",
    "        f_out.write(json.dumps({\"result\": logs}, indent=4))\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_result), exist_ok=True)\n",
    "    with open(output_result, \"a\") as f_out:\n",
    "        f_out.write(json.dumps({\n",
    "            \"model\": args.model_name,\n",
    "            \"seed\": args.seed,\n",
    "            \"auc\": auc_score,\n",
    "            \"auprc\": auprc_score,\n",
    "            \"fnr\": fnr,\n",
    "            \"fpr\": fpr\n",
    "        }) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Prompt Injection Detection Script\")\n",
    "    \n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"qwen-attn\", \n",
    "                        help=\"Path to the model configuration file.\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"deepset/prompt-injections\", \n",
    "                        help=\"Path to the dataset.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    main(args)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T10:59:26.270258Z",
     "start_time": "2025-07-23T10:54:54.144082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "| Provider: attn-hf\n",
      "| Model name: qwen-attn\n",
      "-----------------------\n",
      "===================\n",
      "Using detector: attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [04:24<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.978; AUPRC Score: 0.983; FNR: 0.0; FPR: 0.786\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T11:21:33.069523Z",
     "start_time": "2025-07-23T11:11:49.485333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    output_logs = f\"./result/{args.dataset_name}/{args.model_name}-{args.seed}.json\"\n",
    "    output_result = f\"./result/{args.dataset_name}/result.jsonl\"\n",
    "\n",
    "    model_config_path = f\"configs/model_configs/phi3-mini-attn_config.json\"\n",
    "    model_config = open_config(config_path=model_config_path)\n",
    "\n",
    "    model = create_model(config=model_config)\n",
    "    model.print_model_info()\n",
    "\n",
    "    dataset = load_dataset(\"deepset/prompt-injections\")\n",
    "    test_data = dataset['test']\n",
    "\n",
    "    detector = AttentionDetector(model)\n",
    "    print(\"===================\")\n",
    "    print(f\"Using detector: {detector.name}\")\n",
    "\n",
    "    labels, predictions, scores = [], [], []\n",
    "    logs = []\n",
    "\n",
    "    for data in tqdm(test_data):\n",
    "        result = detector.detect(data['text'])\n",
    "        detect = result[0]\n",
    "        score = result[1]['focus_score']\n",
    "\n",
    "        labels.append(data['label'])\n",
    "        predictions.append(detect)\n",
    "        scores.append(1-score)\n",
    "\n",
    "        result_data = {\n",
    "            \"text\": data['text'],\n",
    "            \"label\": data['label'],\n",
    "            \"result\": result\n",
    "        }\n",
    "\n",
    "        logs.append(result_data)\n",
    "\n",
    "    auc_score = roc_auc_score(labels, scores)\n",
    "    auprc_score = average_precision_score(labels, scores)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    auc_score = round(auc_score, 3)\n",
    "    auprc_score = round(auprc_score, 3)\n",
    "    fnr = round(fnr, 3)\n",
    "    fpr = round(fpr, 3)\n",
    "\n",
    "    print(f\"AUC Score: {auc_score}; AUPRC Score: {auprc_score}; FNR: {fnr}; FPR: {fpr}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_logs), exist_ok=True)\n",
    "    with open(output_logs, \"w\") as f_out:\n",
    "        f_out.write(json.dumps({\"result\": logs}, indent=4))\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_result), exist_ok=True)\n",
    "    with open(output_result, \"a\") as f_out:\n",
    "        f_out.write(json.dumps({\n",
    "            \"model\": args.model_name,\n",
    "            \"seed\": args.seed,\n",
    "            \"auc\": auc_score,\n",
    "            \"auprc\": auprc_score,\n",
    "            \"fnr\": fnr,\n",
    "            \"fpr\": fpr\n",
    "        }) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Prompt Injection Detection Script\")\n",
    "\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"phi3-mini-attn\",\n",
    "                        help=\"Path to the model configuration file.\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"deepset/prompt-injections\",\n",
    "                        help=\"Path to the dataset.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    main(args)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:14<00:00, 67.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "| Provider: attn-hf\n",
      "| Model name: phi3-attn\n",
      "-----------------------\n",
      "===================\n",
      "Using detector: attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/116 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|██████████| 116/116 [07:20<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.915; AUPRC Score: 0.917; FNR: 0.0; FPR: 1.0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_response(model, tokenizer, prompt, max_tokens=50):\n",
    "    \"\"\"Tạo câu trả lời cho prompt không phải injection.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Kịch bản phát hiện Prompt Injection\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"qwen-attn\", \n",
    "                    help=\"Đường dẫn đến tệp cấu hình mô hình.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Nhập query\n",
    "test_query = input(\"Nhập câu query test của bạn: \")\n",
    "\n",
    "# Đặt seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Tải mô hình\n",
    "model_config_path = \"configs/model_configs/qwen2-attn_config.json\"\n",
    "model_config = open_config(config_path=model_config_path)\n",
    "model = create_model(config=model_config)\n",
    "model.print_model_info()\n",
    "\n",
    "# Tạo detector\n",
    "detector = AttentionDetector(model)\n",
    "print(\"===================\")\n",
    "print(f\"Sử dụng detector: {detector.name}\")\n",
    "\n",
    "# Phát hiện\n",
    "result = detector.detect(test_query)\n",
    "is_injection = result[0]\n",
    "focus_score = result[1]['focus_score']\n",
    "\n",
    "print(\"===================\")\n",
    "print(f\"Input: {test_query}\")\n",
    "print(\"Output: \")\n",
    "print(\"Phát hiện prompt injection? \", is_injection)\n",
    "print(\"Điểm focus: \", focus_score)\n",
    "\n",
    "if is_injection:\n",
    "    print(\"CẢNH BÁO: Phát hiện prompt injection tiềm năng! Input có thể là độc hại và sẽ không được xử lý.\")\n",
    "else:\n",
    "    print(\"Câu trả lời: \")\n",
    "    response = generate_response(model.model, model.tokenizer, test_query, max_tokens=model.max_output_tokens)\n",
    "    print(response)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T10:34:10.936821Z",
     "start_time": "2025-07-23T10:33:59.123354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "| Provider: attn-hf\n",
      "| Model name: qwen-attn\n",
      "-----------------------\n",
      "===================\n",
      "Sử dụng detector: attention\n",
      "===================\n",
      "Input: what is machine learning\n",
      "Output: \n",
      "Phát hiện prompt injection?  True\n",
      "Điểm focus:  0.4545251413115433\n",
      "CẢNH BÁO: Phát hiện prompt injection tiềm năng! Input có thể là độc hại và sẽ không được xử lý.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    output_logs = f\"./result/{args.dataset_name}/{args.model_name}-{args.seed}.json\"\n",
    "    output_result = f\"./result/{args.dataset_name}/result.jsonl\"\n",
    "    \n",
    "    model_config_path = f\"configs/model_configs/granite3_8b-attn_config.json\"\n",
    "    model_config = open_config(config_path=model_config_path)\n",
    "\n",
    "    model = create_model(config=model_config)\n",
    "    model.print_model_info()\n",
    "\n",
    "    dataset = load_dataset(\"deepset/prompt-injections\")\n",
    "    test_data = dataset['test']\n",
    "    \n",
    "    detector = AttentionDetector(model)\n",
    "    print(\"===================\")\n",
    "    print(f\"Using detector: {detector.name}\")\n",
    "\n",
    "    labels, predictions, scores = [], [], []\n",
    "    logs = []\n",
    "\n",
    "    for data in tqdm(test_data):\n",
    "        result = detector.detect(data['text'])\n",
    "        detect = result[0]\n",
    "        score = result[1]['focus_score']\n",
    "\n",
    "        labels.append(data['label'])\n",
    "        predictions.append(detect)\n",
    "        scores.append(1-score)\n",
    "\n",
    "        result_data = {\n",
    "            \"text\": data['text'],\n",
    "            \"label\": data['label'],\n",
    "            \"result\": result\n",
    "        }\n",
    "\n",
    "        logs.append(result_data)\n",
    "\n",
    "    auc_score = roc_auc_score(labels, scores)\n",
    "    auprc_score = average_precision_score(labels, scores)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    auc_score = round(auc_score, 3)\n",
    "    auprc_score = round(auprc_score, 3)\n",
    "    fnr = round(fnr, 3)\n",
    "    fpr = round(fpr, 3)\n",
    "\n",
    "    print(f\"AUC Score: {auc_score}; AUPRC Score: {auprc_score}; FNR: {fnr}; FPR: {fpr}\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_logs), exist_ok=True)\n",
    "    with open(output_logs, \"w\") as f_out:\n",
    "        f_out.write(json.dumps({\"result\": logs}, indent=4))\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_result), exist_ok=True)\n",
    "    with open(output_result, \"a\") as f_out:\n",
    "        f_out.write(json.dumps({\n",
    "            \"model\": args.model_name,\n",
    "            \"seed\": args.seed,\n",
    "            \"auc\": auc_score,\n",
    "            \"auprc\": auprc_score,\n",
    "            \"fnr\": fnr,\n",
    "            \"fpr\": fpr\n",
    "        }) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Prompt Injection Detection Script\")\n",
    "    \n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"granite3\",\n",
    "                        help=\"Path to the model configuration file.\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"deepset/prompt-injections\", \n",
    "                        help=\"Path to the dataset.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    main(args)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T11:55:32.530088Z",
     "start_time": "2025-07-23T11:29:03.448746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| Provider: attn-hf\n",
      "| Model name: granite3-8b-attn\n",
      "------------------------------\n",
      "===================\n",
      "Using detector: attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [26:19<00:00, 13.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.996; AUPRC Score: 0.996; FNR: 0.0; FPR: 0.696\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_response(model, tokenizer, prompt, max_tokens=50):\n",
    "    \"\"\"Tạo câu trả lời cho prompt không phải injection.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Kịch bản phát hiện Prompt Injection\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"qwen-attn\", \n",
    "                    help=\"Đường dẫn đến tệp cấu hình mô hình.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Nhập query\n",
    "test_query = input(\"Nhập câu query test của bạn: \")\n",
    "\n",
    "# Đặt seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Tải mô hình\n",
    "model_config_path = \"configs/model_configs/granite3_8b-attn_config.json\"\n",
    "model_config = open_config(config_path=model_config_path)\n",
    "model = create_model(config=model_config)\n",
    "model.print_model_info()\n",
    "\n",
    "# Tạo detector\n",
    "detector = AttentionDetector(model)\n",
    "print(\"===================\")\n",
    "print(f\"Sử dụng detector: {detector.name}\")\n",
    "\n",
    "# Phát hiện\n",
    "result = detector.detect(test_query)\n",
    "is_injection = result[0]\n",
    "focus_score = result[1]['focus_score']\n",
    "\n",
    "print(\"===================\")\n",
    "print(f\"Input: {test_query}\")\n",
    "print(\"Output: \")\n",
    "print(\"Phát hiện prompt injection? \", is_injection)\n",
    "print(\"Điểm focus: \", focus_score)\n",
    "\n",
    "if is_injection:\n",
    "    print(\"CẢNH BÁO: Phát hiện prompt injection tiềm năng! Input có thể là độc hại và sẽ không được xử lý.\")\n",
    "else:\n",
    "    print(\"Câu trả lời: \")\n",
    "    response = generate_response(model.model, model.tokenizer, test_query, max_tokens=model.max_output_tokens)\n",
    "    print(response)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T12:33:18.482950Z",
     "start_time": "2025-07-23T12:27:19.258084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 52.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| Provider: attn-hf\n",
      "| Model name: granite3-8b-attn\n",
      "------------------------------\n",
      "===================\n",
      "Sử dụng detector: attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Input: the ocean is blue\n",
      "Output: \n",
      "Phát hiện prompt injection?  False\n",
      "Điểm focus:  0.7635254074887532\n",
      "Câu trả lời: \n",
      "systemKnowledge Cutoff Date: April 2024.\n",
      "Today's Date: July 23, 2025.\n",
      "You are Granite, developed by IBM. You are a helpful AI assistant.\n",
      "userthe ocean is blue\n",
      "assistantYes, that's correct. The ocean appears blue due to the absorption and scattering of light. Water absorbs longer wavelengths of light, such as red and orange, more quickly than shorter wavelengths like blue and violet.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ]
}
