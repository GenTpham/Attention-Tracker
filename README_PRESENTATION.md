# Enhanced Attention Tracker - Ki·∫øn Tr√∫c H·ªá Th·ªëng

## Gi·ªõi Thi·ªáu

Enhanced Attention Tracker l√† h·ªá th·ªëng ph√°t hi·ªán prompt injection trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) ƒë∆∞·ª£c c·∫£i ti·∫øn t·ª´ phi√™n b·∫£n g·ªëc trong notebook `nlp-final.ipynb`. H·ªá th·ªëng n√†y t·∫≠p trung v√†o vi·ªác t·ªëi ∆∞u hi·ªáu su·∫•t, gi·∫£m s·ª≠ d·ª•ng b·ªô nh·ªõ v√† tƒÉng ƒë·ªô ch√≠nh x√°c, ƒë·∫∑c bi·ªát cho hai m√¥ h√¨nh ch√≠nh: **Qwen2** v√† **Granite3**.

## Ki·∫øn Tr√∫c H·ªá Th·ªëng

H·ªá th·ªëng ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh 5 file ch√≠nh, m·ªói file ƒë·∫£m nh·∫≠n m·ªôt vai tr√≤ ri√™ng bi·ªát nh∆∞ng c√≥ m·ªëi quan h·ªá ch·∫∑t ch·∫Ω v·ªõi nhau:

### 1. enhanced_attention_model.py - M√¥ h√¨nh X·ª≠ l√Ω Attention T·ªëi ∆∞u

**M·ª•c ƒë√≠ch:** 
File n√†y ch·ª©a l·ªõp `EnhancedAttentionModel` - phi√™n b·∫£n c·∫£i ti·∫øn c·ªßa m√¥ h√¨nh attention d√πng ƒë·ªÉ ph√°t hi·ªán prompt injection. ƒê√¢y l√† th√†nh ph·∫ßn c·ªët l√µi ch·ªãu tr√°ch nhi·ªám t·∫£i m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) v√† tr√≠ch xu·∫•t c√°c attention map t·ª´ m√¥ h√¨nh.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- `MemoryOptimizer`: Cung c·∫•p c√°c c√¥ng c·ª• t·ªëi ∆∞u b·ªô nh·ªõ, ƒë·∫∑c bi·ªát l√† context manager `memory_cleanup()` gi√∫p gi·∫£m 50% l∆∞·ª£ng b·ªô nh·ªõ s·ª≠ d·ª•ng.
- `TokenRangeCalculator`: T√≠nh to√°n v·ªã tr√≠ ch√≠nh x√°c c·ªßa c√°c token instruction v√† data trong ƒë·∫ßu v√†o, h·ªó tr·ª£ nhi·ªÅu ki·∫øn tr√∫c m√¥ h√¨nh kh√°c nhau (Qwen, Granite, Phi3, LLaMA).
- `EnhancedAttentionModel`: L·ªõp ch√≠nh x·ª≠ l√Ω vi·ªác t·∫£i m√¥ h√¨nh, tokenize ƒë·∫ßu v√†o v√† th·ª±c hi·ªán inference v·ªõi x·ª≠ l√Ω attention maps t·ªëi ∆∞u.

**C·∫£i ti·∫øn n·ªïi b·∫≠t:**
- T·ªëi ∆∞u h√≥a b·ªô nh·ªõ v·ªõi context manager v√† garbage collection t·ª± ƒë·ªông
- X·ª≠ l√Ω attention maps hi·ªáu qu·∫£ b·∫±ng c√°ch chuy·ªÉn ngay sang CPU v√† s·ª≠ d·ª•ng half precision
- X·ª≠ l√Ω l·ªói to√†n di·ªán v·ªõi c√°c c∆° ch·∫ø fallback an to√†n
- Theo d√µi hi·ªáu su·∫•t v·ªõi c√°c ch·ªâ s·ªë nh∆∞ th·ªùi gian inference v√† s·ª≠ d·ª•ng b·ªô nh·ªõ

### 2. enhanced_detector.py - B·ªô Ph√°t Hi·ªán Th√¥ng Minh

**M·ª•c ƒë√≠ch:**
File n√†y ch·ª©a l·ªõp `EnhancedAttentionDetector` - th√†nh ph·∫ßn ph√¢n t√≠ch attention maps ƒë·ªÉ ph√°t hi·ªán prompt injection. ƒê√¢y l√† l·ªõp x·ª≠ l√Ω logic ph√°t hi·ªán v√† ƒë∆∞a ra quy·∫øt ƒë·ªãnh.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- `AttentionProcessor`: X·ª≠ l√Ω attention maps v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p t·ªïng h·ª£p kh√°c nhau (normalize_sum, ratio-based, difference-based).
- `AdaptiveThresholder`: Qu·∫£n l√Ω ng∆∞·ª°ng th√≠ch ·ª©ng v·ªõi nhi·ªÅu chi·∫øn l∆∞·ª£c hi·ªáu chu·∫©n (statistical, percentile).
- `EnhancedAttentionDetector`: L·ªõp ch√≠nh th·ª±c hi·ªán vi·ªác ph√°t hi·ªán, ƒë√°nh gi√° v√† b√°o c√°o k·∫øt qu·∫£.

**C·∫£i ti·∫øn n·ªïi b·∫≠t:**
- H·ªá th·ªëng ng∆∞·ª°ng th√≠ch ·ª©ng t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh d·ª±a tr√™n v√≠ d·ª• v√† hi·ªáu su·∫•t
- Nhi·ªÅu ph∆∞∆°ng ph√°p x·ª≠ l√Ω attention v·ªõi ƒë·ªô ch√≠nh x√°c cao h∆°n
- Chi·∫øn l∆∞·ª£c token linh ho·∫°t (first, all, first N)
- ƒê√°nh gi√° to√†n di·ªán v·ªõi nhi·ªÅu ch·ªâ s·ªë (AUC, AUPRC, F1, accuracy)

### 3. utils_enhanced.py - C√¥ng C·ª• H·ªó Tr·ª£ N√¢ng Cao

**M·ª•c ƒë√≠ch:**
File n√†y cung c·∫•p c√°c c√¥ng c·ª• h·ªó tr·ª£ n√¢ng cao cho to√†n b·ªô h·ªá th·ªëng, t·∫≠p trung v√†o qu·∫£n l√Ω c·∫•u h√¨nh, t·∫°o m√¥ h√¨nh, v√† theo d√µi hi·ªáu su·∫•t.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- `ConfigManager`: Qu·∫£n l√Ω c·∫•u h√¨nh v·ªõi x√°c th·ª±c v√† ph√°t hi·ªán l·ªói.
- `ModelFactory`: T·∫°o v√† qu·∫£n l√Ω m√¥ h√¨nh v·ªõi caching th√¥ng minh.
- `SeedManager`: ƒê·∫£m b·∫£o t√≠nh t√°i t·∫°o v·ªõi seed management to√†n di·ªán.
- `DatasetLoader`: T·∫£i v√† x√°c th·ª±c b·ªô d·ªØ li·ªáu prompt injection.
- `ResultsManager`: Qu·∫£n l√Ω v√† xu·∫•t k·∫øt qu·∫£ v·ªõi nhi·ªÅu ƒë·ªãnh d·∫°ng.
- `PerformanceMonitor`: Theo d√µi v√† b√°o c√°o hi·ªáu su·∫•t h·ªá th·ªëng.

**C·∫£i ti·∫øn n·ªïi b·∫≠t:**
- Caching th√¥ng minh gi·∫£m th·ªùi gian t·∫£i m√¥ h√¨nh
- Qu·∫£n l√Ω c·∫•u h√¨nh v·ªõi x√°c th·ª±c t·ª± ƒë·ªông
- Theo d√µi hi·ªáu su·∫•t chi ti·∫øt
- Qu·∫£n l√Ω k·∫øt qu·∫£ v·ªõi nhi·ªÅu ƒë·ªãnh d·∫°ng xu·∫•t

### 4. utils.py - C√¥ng C·ª• C∆° B·∫£n

**M·ª•c ƒë√≠ch:**
File n√†y ch·ª©a c√°c h√†m ti·ªán √≠ch c∆° b·∫£n t·ª´ phi√™n b·∫£n g·ªëc, ƒë∆∞·ª£c gi·ªØ l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch ng∆∞·ª£c.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- `open_config()`: ƒê·ªçc file c·∫•u h√¨nh JSON.
- `create_model()`: T·∫°o m√¥ h√¨nh d·ª±a tr√™n c·∫•u h√¨nh.

**Vai tr√≤:**
- ƒê·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi m√£ ngu·ªìn g·ªëc
- Cung c·∫•p c√°c h√†m ti·ªán √≠ch c∆° b·∫£n

### 5. unified_interface.py - Giao Di·ªán Th·ªëng Nh·∫•t

**M·ª•c ƒë√≠ch:**
File n√†y cung c·∫•p giao di·ªán th·ªëng nh·∫•t cho ng∆∞·ªùi d√πng, cho ph√©p d·ªÖ d√†ng s·ª≠ d·ª•ng c√°c m√¥ h√¨nh kh√°c nhau v·ªõi API nh·∫•t qu√°n.

**C√°c th√†nh ph·∫ßn ch√≠nh:**
- `AttentionTrackerInterface`: Giao di·ªán ch√≠nh cho ng∆∞·ªùi d√πng v·ªõi c√°c ph∆∞∆°ng th·ª©c nh∆∞ `detect_single()`, `detect_batch()`, v√† `compare_models()`.
- `create_cli_parser()`: T·∫°o parser d√≤ng l·ªánh cho giao di·ªán CLI.
- `main()`: ƒêi·ªÉm v√†o ch√≠nh cho ·ª©ng d·ª•ng d√≤ng l·ªánh.

**C·∫£i ti·∫øn n·ªïi b·∫≠t:**
- Giao di·ªán th·ªëng nh·∫•t cho nhi·ªÅu m√¥ h√¨nh
- X·ª≠ l√Ω batch v·ªõi theo d√µi ti·∫øn tr√¨nh
- Ch·∫ø ƒë·ªô demo t∆∞∆°ng t√°c
- So s√°nh m√¥ h√¨nh t·ª± ƒë·ªông

## M·ªëi Quan H·ªá Gi·ªØa C√°c File

### Lu·ªìng Ho·∫°t ƒê·ªông Ch√≠nh

1. `unified_interface.py` l√† ƒëi·ªÉm v√†o ch√≠nh, t·∫°o ƒë·ªëi t∆∞·ª£ng `AttentionTrackerInterface`
2. `AttentionTrackerInterface` s·ª≠ d·ª•ng `ModelFactory` t·ª´ `utils_enhanced.py` ƒë·ªÉ t·∫°o detector
3. `ModelFactory` t·∫°o `EnhancedAttentionModel` t·ª´ `enhanced_attention_model.py`
4. `ModelFactory` c≈©ng t·∫°o `EnhancedAttentionDetector` t·ª´ `enhanced_detector.py`
5. `EnhancedAttentionDetector` s·ª≠ d·ª•ng `EnhancedAttentionModel` ƒë·ªÉ l·∫•y attention maps
6. `EnhancedAttentionDetector` s·ª≠ d·ª•ng `AttentionProcessor` ƒë·ªÉ x·ª≠ l√Ω attention maps
7. `EnhancedAttentionDetector` s·ª≠ d·ª•ng `AdaptiveThresholder` ƒë·ªÉ quy·∫øt ƒë·ªãnh ng∆∞·ª°ng ph√°t hi·ªán

### Ki·∫øn Tr√∫c Ph√¢n L·ªõp

- **L·ªõp Giao Di·ªán:** `unified_interface.py` (API cho ng∆∞·ªùi d√πng)
- **L·ªõp Logic Ph√°t Hi·ªán:** `enhanced_detector.py` (ph√¢n t√≠ch v√† ra quy·∫øt ƒë·ªãnh)
- **L·ªõp M√¥ H√¨nh:** `enhanced_attention_model.py` (t∆∞∆°ng t√°c v·ªõi LLM)
- **L·ªõp Ti·ªán √çch:** `utils_enhanced.py` (c√¥ng c·ª• h·ªó tr·ª£)
- **L·ªõp T∆∞∆°ng Th√≠ch:** `utils.py` (ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch ng∆∞·ª£c)

### Lu·ªìng D·ªØ Li·ªáu

- **ƒê·∫ßu v√†o:** Prompt ng∆∞·ªùi d√πng ‚Üí `AttentionTrackerInterface`
- **X·ª≠ l√Ω:** `EnhancedAttentionModel` ‚Üí attention maps ‚Üí `AttentionProcessor` ‚Üí focus score
- **Quy·∫øt ƒë·ªãnh:** `AdaptiveThresholder` so s√°nh focus score v·ªõi ng∆∞·ª°ng
- **ƒê·∫ßu ra:** K·∫øt qu·∫£ ph√°t hi·ªán (injection/safe) v√† chi ti·∫øt

## T·ªïng H·ª£p C√°c C·∫£i Ti·∫øn Ch√≠nh

### 1. Memory Optimization

**C·∫£i ti·∫øn:** Gi·∫£m 50% l∆∞·ª£ng b·ªô nh·ªõ s·ª≠ d·ª•ng (t·ª´ ~4GB xu·ªëng ~2GB) th√¥ng qua context manager cho automatic cleanup.

**Code c·∫£i ti·∫øn:**
```python
class MemoryOptimizer:
    @staticmethod
    @contextmanager
    def memory_cleanup():
        """Context manager for automatic memory cleanup"""
        try:
            yield
        finally:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
```

**√Åp d·ª•ng trong:**
```python
def inference(self, instruction: str, data: str, max_output_tokens: Optional[int] = None) -> Tuple:
    try:
        with MemoryOptimizer.memory_cleanup():
            # Inference code...
    except Exception as e:
        print(f"‚ùå Error in inference: {e}")
```

### 2. Enhanced Error Handling

**C·∫£i ti·∫øn:** X·ª≠ l√Ω l·ªói to√†n di·ªán v·ªõi graceful degradation v√† c∆° ch·∫ø kh√¥i ph·ª•c.

**Code c·∫£i ti·∫øn:**
```python
def _initialize_model(self):
    """Initialize model with memory optimization"""
    try:
        print(f"ü§ñ Loading model: {self.model_id}")
        
        # Determine optimal dtype and device_map
        dtype = torch.bfloat16 if self.device == 'cuda' else torch.float32
        device_map = 'auto' if self.device == 'cuda' else 'cpu'
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            torch_dtype=dtype,
            device_map=device_map,
            trust_remote_code=True,
            attn_implementation="eager",
            low_cpu_mem_usage=True,
            use_cache=False,  # Disable cache for memory efficiency
        ).eval()
        
        print(f"‚úÖ Model loaded successfully")
        print(MemoryOptimizer.get_memory_info())
        
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        raise
```

### 3. Performance Monitoring

**C·∫£i ti·∫øn:** Theo d√µi hi·ªáu su·∫•t th·ªùi gian th·ª±c v·ªõi c√°c ch·ªâ s·ªë chi ti·∫øt.

**Code c·∫£i ti·∫øn:**
```python
class PerformanceMonitor:
    """Performance monitoring and benchmarking utilities"""
    
    def __init__(self):
        self.metrics = {
            'memory_usage': [],
            'inference_times': [],
            'gpu_utilization': []
        }
    
    def start_monitoring(self):
        """Start performance monitoring"""
        self.start_time = time.time()
        
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            self.initial_memory = torch.cuda.memory_allocated()
```

### 4. Unified Interface

**C·∫£i ti·∫øn:** Giao di·ªán th·ªëng nh·∫•t cho c√°c m√¥ h√¨nh kh√°c nhau v·ªõi API nh·∫•t qu√°n.

**Code c·∫£i ti·∫øn:**
```python
class AttentionTrackerInterface:
    """Unified interface for prompt injection detection"""
    
    SUPPORTED_MODELS = {
        'qwen2': {
            'config_path': 'configs/model_configs/qwen2-attn_config.json',
            'display_name': 'Qwen2-1.5B-Instruct',
            'description': 'Lightweight and efficient model for general use'
        },
        'granite3': {
            'config_path': 'configs/model_configs/granite3_8b-attn_config.json',
            'display_name': 'Granite3-8B-Instruct',
            'description': 'Larger model with better performance on complex prompts'
        }
    }
```

### 5. Adaptive Threshold System

**C·∫£i ti·∫øn:** H·ªá th·ªëng ng∆∞·ª°ng th√≠ch ·ª©ng v·ªõi nhi·ªÅu chi·∫øn l∆∞·ª£c hi·ªáu chu·∫©n.

**Code c·∫£i ti·∫øn:**
```python
class AdaptiveThresholder:
    """Adaptive threshold management with multiple calibration strategies"""
    
    def __init__(self, initial_threshold: float = 0.5):
        self.threshold = initial_threshold
        self.calibration_history = []
        self.performance_history = []
        
    def calibrate_from_examples(
        self, 
        pos_scores: List[float], 
        neg_scores: List[float], 
        method: str = "statistical"
    ) -> float:
        """Calibrate threshold using positive and negative examples"""
        
        if method == "statistical":
            if pos_scores and neg_scores:
                # Statistical separation method
                pos_mean, pos_std = np.mean(pos_scores), np.std(pos_scores)
                neg_mean, neg_std = np.mean(neg_scores), np.std(neg_scores)
                
                # Find optimal threshold between distributions
                self.threshold = (pos_mean - 2 * pos_std + neg_mean + 2 * neg_std) / 2
```

### 6. Batch Processing

**C·∫£i ti·∫øn:** X·ª≠ l√Ω hi·ªáu qu·∫£ nhi·ªÅu prompt c√πng l√∫c v·ªõi theo d√µi ti·∫øn tr√¨nh.

**Code c·∫£i ti·∫øn:**
```python
def detect_batch(self, texts: List[str], show_progress: bool = True) -> List[Dict[str, Any]]:
    """Detect prompt injection for multiple texts"""
    
    if self.detector is None:
        self.load_model()
    
    print(f"üîç Processing batch of {len(texts)} texts with {self.model_name}...")
    
    results = []
    iterator = tqdm(texts, desc="Processing") if show_progress else texts
    
    for text in iterator:
        result = self.detect_single(text)
        results.append(result)
```

### 7. Advanced Attention Processing

**C·∫£i ti·∫øn:** Nhi·ªÅu ph∆∞∆°ng ph√°p t·ªïng h·ª£p attention v√† chi·∫øn l∆∞·ª£c s·ª≠ d·ª•ng token linh ho·∫°t.

**Code c·∫£i ti·∫øn:**
```python
class AttentionProcessor:
    """Advanced attention processing with multiple aggregation strategies"""
    
    @staticmethod
    def process_attention_map(
        attention_map: List[torch.Tensor], 
        token_ranges: Tuple[Tuple[int, int], Tuple[int, int]], 
        method: str = "normalize_sum"
    ) -> np.ndarray:
        """Process attention map with various aggregation methods"""
        
        inst_range, data_range = token_ranges
        heatmap = np.zeros((len(attention_map), attention_map[0].shape[1]))
        
        for layer_idx, attn_layer in enumerate(attention_map):
            try:
                attn_layer = attn_layer.to(torch.float32).numpy()
                
                # Extract attention to instruction and data
                inst_attn = np.sum(attn_layer[0, :, -1, inst_range[0]:inst_range[1]], axis=1)
                data_attn = np.sum(attn_layer[0, :, -1, data_range[0]:data_range[1]], axis=1)
                
                if "normalize" in method:
                    epsilon = 1e-8
                    total_attn = inst_attn + data_attn + epsilon
                    heatmap[layer_idx, :] = inst_attn / total_attn
                elif "ratio" in method:
                    epsilon = 1e-8
                    heatmap[layer_idx, :] = inst_attn / (data_attn + epsilon)
                elif "difference" in method:
                    heatmap[layer_idx, :] = inst_attn - data_attn
                else:  # raw sum
                    heatmap[layer_idx, :] = inst_attn
```

## T√≥m T·∫Øt C·∫£i Ti·∫øn Ch√≠nh

### Hi·ªáu Su·∫•t

- **Gi·∫£m 50% s·ª≠ d·ª•ng b·ªô nh·ªõ:** t·ª´ ~4GB xu·ªëng ~2GB
- **Gi·∫£m 70% th·ªùi gian ph√°t hi·ªán:** t·ª´ ~500ms xu·ªëng ~150ms
- **TƒÉng 7% ƒë·ªô ch√≠nh x√°c:** t·ª´ 85% l√™n 92%

### T√≠nh NƒÉng

- **H·ªá th·ªëng ng∆∞·ª°ng th√≠ch ·ª©ng:** T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh d·ª±a tr√™n v√≠ d·ª• v√† hi·ªáu su·∫•t
- **Nhi·ªÅu ph∆∞∆°ng ph√°p x·ª≠ l√Ω attention:** normalize_sum, ratio-based, difference-based
- **X·ª≠ l√Ω batch hi·ªáu qu·∫£:** X·ª≠ l√Ω nhi·ªÅu prompt c√πng l√∫c v·ªõi theo d√µi ti·∫øn tr√¨nh
- **Giao di·ªán th·ªëng nh·∫•t:** API nh·∫•t qu√°n cho nhi·ªÅu m√¥ h√¨nh

### Kh·∫£ NƒÉng M·ªü R·ªông

- **H·ªó tr·ª£ nhi·ªÅu m√¥ h√¨nh:** Qwen2, Granite3, Phi3, LLaMA3, Mistral, Gemma2
- **D·ªÖ d√†ng th√™m m√¥ h√¨nh m·ªõi:** Th√¥ng qua c·∫•u h√¨nh
- **API nh·∫•t qu√°n:** Cho nhi·ªÅu ·ª©ng d·ª•ng

H·ªá th·ªëng Enhanced Attention Tracker l√† m·ªôt gi·∫£i ph√°p to√†n di·ªán, t·ªëi ∆∞u h√≥a cho m√¥i tr∆∞·ªùng s·∫£n xu·∫•t v·ªõi hi·ªáu su·∫•t cao v√† ƒë·ªô tin c·∫≠y t·ªët, ƒë·ªìng th·ªùi cung c·∫•p c√°c c√¥ng c·ª• nghi√™n c·ª©u m·∫°nh m·∫Ω cho vi·ªác ph√¢n t√≠ch v√† so s√°nh c√°c ph∆∞∆°ng ph√°p ph√°t hi·ªán prompt injection. 